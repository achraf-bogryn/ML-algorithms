{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Tokenisation Text-Procssing Methode\n",
        "\n",
        "* `NLTK`: Natural Language Toolkit .\n",
        "* `Spacy`: Industrial-Strength Natural Language Processing"
      ],
      "metadata": {
        "id": "qidnk-oPDdai"
      }
    },
    {
      "source": [
        "import nltk\n",
        "\n",
        "nltk.download('punkt_tab')"
      ],
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w76gUygPGiXu",
        "outputId": "04d70832-204d-47e0-f795-da61a70030ee"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G5guAkvkDWF1",
        "outputId": "0d5b60bc-1610-459a-f008-5a292078477b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3.9.1\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "print(nltk.__version__)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "corpus= \"\"\"Hello everyone I'm achraf and to my learning journey in NLP and data science tutorial. by doing fun projcts and build useful models.\n",
        "please give your feedbacks becuase courage me to do better job.\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "bGfjXznkE5Hj"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(corpus)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LRpKzSW3FS4X",
        "outputId": "6237e3f1-4718-4d99-902a-a339fa1ff891"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hello everyone I'm achraf and to my learning journey in NLP and data science tutorial . by doing fun projcts and build useful models .\n",
            "please give your feedbacks becuase courage me to do better job.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Tokenization\n",
        "\n",
        "* paragraphs ---> Sentence"
      ],
      "metadata": {
        "id": "y2g7bQytF1LB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import sent_tokenize\n",
        "documents =sent_tokenize(corpus)"
      ],
      "metadata": {
        "id": "-tqBlOWlGFli"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "type(documents)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3B493KbPHay9",
        "outputId": "e6424ffa-ab3b-4ac0-ab4e-563c88380a35"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "list"
            ]
          },
          "metadata": {},
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for sentence in documents:\n",
        "    print(sentence)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qUCSCsoMHcR-",
        "outputId": "d4988ee6-2d00-45e6-f08a-29f6485133d4"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hello everyone I'm achraf and to my learning journey in NLP and data science tutorial .\n",
            "by doing fun projcts and build useful models .\n",
            "please give your feedbacks becuase courage me to do better job.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Paragraph -->words\n",
        "* sentence  --> words"
      ],
      "metadata": {
        "id": "RtJUtNOAH7ix"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import word_tokenize\n",
        "word_tokenize(corpus)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7oW__w1VIFgQ",
        "outputId": "c0691443-44b1-449b-a8e0-18e28c723f5d"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Hello',\n",
              " 'everyone',\n",
              " 'I',\n",
              " \"'m\",\n",
              " 'achraf',\n",
              " 'and',\n",
              " 'to',\n",
              " 'my',\n",
              " 'learning',\n",
              " 'journey',\n",
              " 'in',\n",
              " 'NLP',\n",
              " 'and',\n",
              " 'data',\n",
              " 'science',\n",
              " 'tutorial',\n",
              " '.',\n",
              " 'by',\n",
              " 'doing',\n",
              " 'fun',\n",
              " 'projcts',\n",
              " 'and',\n",
              " 'build',\n",
              " 'useful',\n",
              " 'models',\n",
              " '.',\n",
              " 'please',\n",
              " 'give',\n",
              " 'your',\n",
              " 'feedbacks',\n",
              " 'becuase',\n",
              " 'courage',\n",
              " 'me',\n",
              " 'to',\n",
              " 'do',\n",
              " 'better',\n",
              " 'job',\n",
              " '.']"
            ]
          },
          "metadata": {},
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for word in documents:\n",
        "    print(word_tokenize(word))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QqImN-j6NUPr",
        "outputId": "c25fceca-777e-4c96-f548-dbf6ab939047"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Hello', 'everyone', 'I', \"'m\", 'achraf', 'and', 'to', 'my', 'learning', 'journey', 'in', 'NLP', 'and', 'data', 'science', 'tutorial', '.']\n",
            "['by', 'doing', 'fun', 'projcts', 'and', 'build', 'useful', 'models', '.']\n",
            "['please', 'give', 'your', 'feedbacks', 'becuase', 'courage', 'me', 'to', 'do', 'better', 'job', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import wordpunct_tokenize\n",
        "wordpunct_tokenize(corpus)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8PT0HUB3Nrop",
        "outputId": "86e71c95-432d-41da-c15c-89c9e3556cb3"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Hello',\n",
              " 'everyone',\n",
              " 'I',\n",
              " \"'\",\n",
              " 'm',\n",
              " 'achraf',\n",
              " 'and',\n",
              " 'to',\n",
              " 'my',\n",
              " 'learning',\n",
              " 'journey',\n",
              " 'in',\n",
              " 'NLP',\n",
              " 'and',\n",
              " 'data',\n",
              " 'science',\n",
              " 'tutorial',\n",
              " '.',\n",
              " 'by',\n",
              " 'doing',\n",
              " 'fun',\n",
              " 'projcts',\n",
              " 'and',\n",
              " 'build',\n",
              " 'useful',\n",
              " 'models',\n",
              " '.',\n",
              " 'please',\n",
              " 'give',\n",
              " 'your',\n",
              " 'feedbacks',\n",
              " 'becuase',\n",
              " 'courage',\n",
              " 'me',\n",
              " 'to',\n",
              " 'do',\n",
              " 'better',\n",
              " 'job',\n",
              " '.']"
            ]
          },
          "metadata": {},
          "execution_count": 50
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import TreebankWordTokenizer\n",
        "tokenizer = TreebankWordTokenizer()\n",
        "tokenizer.tokenize(corpus)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ERLKBuDHOl6I",
        "outputId": "04e9d761-cbf1-4c1c-a731-186293668bf8"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Hello',\n",
              " 'everyone',\n",
              " 'I',\n",
              " \"'m\",\n",
              " 'achraf',\n",
              " 'and',\n",
              " 'to',\n",
              " 'my',\n",
              " 'learning',\n",
              " 'journey',\n",
              " 'in',\n",
              " 'NLP',\n",
              " 'and',\n",
              " 'data',\n",
              " 'science',\n",
              " 'tutorial.',\n",
              " 'by',\n",
              " 'doing',\n",
              " 'fun',\n",
              " 'projcts',\n",
              " 'and',\n",
              " 'build',\n",
              " 'useful',\n",
              " 'models.',\n",
              " 'please',\n",
              " 'give',\n",
              " 'your',\n",
              " 'feedbacks',\n",
              " 'becuase',\n",
              " 'courage',\n",
              " 'me',\n",
              " 'to',\n",
              " 'do',\n",
              " 'better',\n",
              " 'job',\n",
              " '.']"
            ]
          },
          "metadata": {},
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "differnt between `word_tokenize` and `wordpunct_tokenize` in last useful for word that contains apostrof ('s) he separate beween word and apostrof, and `TreebankWordTokenizer`for . stop end of sentence don't count as separate words ."
      ],
      "metadata": {
        "id": "WaZhHTHuN1dl"
      }
    }
  ]
}